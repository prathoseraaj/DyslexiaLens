{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3be1973c",
   "metadata": {},
   "source": [
    "DyslexiaLens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1437d4",
   "metadata": {},
   "source": [
    "Text Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d6e6dd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#text = input(Text Input: )\n",
    "text = \"\"\"The defendant shall constitute a response to the aforementioned allegations \n",
    "    pursuant to the statutory requirements, whereas failure to comply may result \n",
    "    in default judgment.\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019e65f5",
   "metadata": {},
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f4e567",
   "metadata": {},
   "source": [
    "1. Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "82ff3075",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ftfy\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "dde79d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "\n",
    "    #to clear the encoding issues\n",
    "    text = ftfy.fix_text(text)\n",
    "\n",
    "    #to remove the non printable letters\n",
    "    text = ''.join(c for c in text if c.isprintable())\n",
    "    \n",
    "    #to remove the extra spaces\n",
    "    text = re.sub(r'[\\r\\n]+', '\\n', text)\n",
    "    text = re.sub(r'\\s{2,}', ' ', text)\n",
    "\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "452b0cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanText = clean_text(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8557341c",
   "metadata": {},
   "source": [
    "2. Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1b9d3c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/prathoseraaj-v/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk \n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "24c45f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segmentation_text(text):\n",
    "    paragraph = [p for p in text.split('\\n') if p.strip()]\n",
    "    sentance = []\n",
    "\n",
    "    for para in paragraph:\n",
    "        sentance.extend(nltk.sent_tokenize(para))\n",
    "\n",
    "    tokens = [nltk.word_tokenize(sent) for sent in sentance]\n",
    "    return {\n",
    "        'paragraph': paragraph,\n",
    "        'sentences': sentance,\n",
    "        'tokens': tokens,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4936eff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_text = segmentation_text(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6e782c",
   "metadata": {},
   "source": [
    "Readability Assesment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "91873536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The defendant shall constitute a response to the aforementioned allegations      pursuant to the statutory requirements, whereas failure to comply may result      in default judgment.\n",
      "['The defendant shall constitute a response to the aforementioned allegations', '    pursuant to the statutory requirements, whereas failure to comply may result', '    in default judgment.']\n"
     ]
    }
   ],
   "source": [
    "full_text = \" \".join(preprocessed_text[\"paragraph\"])\n",
    "sentences = preprocessed_text[\"sentences\"]\n",
    "\n",
    "print(full_text)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8975d9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0a8ae578",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26940d6",
   "metadata": {},
   "source": [
    "1. Readability Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "40c7d5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "fb5e2523",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readability_score(text):\n",
    "    return{\n",
    "        \"flesch_reading_ease\" : textstat.flesch_reading_ease(text),\n",
    "        \"flesch_kincaid_grade\" : textstat.flesch_kincaid_grade(text),\n",
    "        \"gunning_fog\": textstat.gunning_fog(text),\n",
    "        \"smog_index\": textstat.smog_index(text),\n",
    "        \"coleman_liau_index\" : textstat.coleman_liau_index(text),\n",
    "        \"automated_readability_index\" : textstat.automated_readability_index(text),\n",
    "        \"dale_chall_readability_score\" : textstat.dale_chall_readability_score(text),\n",
    "        \"difficult_words_count\" : textstat.difficult_words(text),\n",
    "        \"difficult_words_list\" : textstat.difficult_words_list(text),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0dfd7f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'flesch_reading_ease': 13.275000000000034,\n",
       " 'flesch_kincaid_grade': 17.37,\n",
       " 'gunning_fog': 21.26666666666667,\n",
       " 'smog_index': 18.243605946275583,\n",
       " 'coleman_liau_index': 18.733333333333338,\n",
       " 'automated_readability_index': 20.0075,\n",
       " 'dale_chall_readability_score': 14.037733333333335,\n",
       " 'difficult_words_count': 14,\n",
       " 'difficult_words_list': ['response',\n",
       "  'pursuant',\n",
       "  'allegations',\n",
       "  'requirements',\n",
       "  'failure',\n",
       "  'default',\n",
       "  'aforementioned',\n",
       "  'judgment',\n",
       "  'result',\n",
       "  'comply',\n",
       "  'defendant',\n",
       "  'whereas',\n",
       "  'constitute',\n",
       "  'statutory']}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "readability_score(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39ed2ec",
   "metadata": {},
   "source": [
    "2. Detect long sentance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d22a4127",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_long_sentance(sentances, threshold=25):\n",
    "    return [sent for sent in sentances if len(sent.split()) > threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8565580d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detect_long_sentance(sentances=sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75e5dcf",
   "metadata": {},
   "source": [
    "3. Detect passive voice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c113e96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4f3c8dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_passive_voice(sentences):\n",
    "    passive_sentences = []\n",
    "\n",
    "    for sent in sentences:\n",
    "        doc = nlp(sent)\n",
    "        for token in doc:\n",
    "            if token.dep_ == \"nsubjpass\":\n",
    "                passive_sentences.append(sent)\n",
    "                break\n",
    "\n",
    "    return passive_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "696b0ff9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detect_long_sentance(sentances=sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e4b22f",
   "metadata": {},
   "source": [
    "4. Detect ambigious structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "045c2f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_ambiguous_structures(sentences):\n",
    "    ambiguous_keywords = [\"might\", \"could\", \"possibly\", \"maybe\", \"potentially\", \"approximately\", \"suggests\", \"appears\"]\n",
    "    return [sent for sent in sentences if any(word in sent.lower() for word in ambiguous_keywords)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "674a68e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detect_ambiguous_structures(sentences=sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ee6580",
   "metadata": {},
   "source": [
    "5. Return the readability assesment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "1830b6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assesment_data(preprocessed_text):\n",
    "\n",
    "    paragraph = \"\".join(preprocessed_text['paragraph'])\n",
    "    sentences = preprocessed_text[\"sentences\"]\n",
    "\n",
    "    return{\n",
    "        \"readability_score\" : readability_score(paragraph),\n",
    "        \"long_sentences\" : detect_long_sentance(sentences),\n",
    "        \"passive_voice\" : detect_passive_voice(sentences),\n",
    "        \"detect_ambiguous_structures\" : detect_ambiguous_structures(sentences),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "20a162a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = assesment_data(preprocessed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45f4cdb",
   "metadata": {},
   "source": [
    "Text Simplification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "3268eff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/prathoseraaj-v/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 Loading AI simplification model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model loaded successfully!\n",
      "\n",
      "=== AI-POWERED DYSLEXIALENS SIMPLIFICATION ===\n",
      "Original text grade level: 29.3\n",
      "Difficult words: 40\n",
      "\n",
      "🤖 Processing with AI model...\n",
      "\n",
      "=== SIMPLIFIED TEXT ===\n",
      "1. Pursuant to the overarching objectives articulated within the Strategic Urban Development Framework, municipal authorities have commenced implementation of multifaceted infrastructural initiatives intended to augment both intermodal connectivity and socioeconomic inclusivity.\n",
      "2. The preliminary phase encompasses the recalibration of extant transportation modalities, the optimization of resource allocation protocols, and the systemic integration of public-private partnerships to facilitate sustainable urban growth trajectories.\n",
      "\n",
      "=== RESULTS ===\n",
      "Original grade level: 29.3\n",
      "Simplified grade level: 29.3\n",
      "Improvement: 0.0 grades easier\n",
      "Difficult words reduced: 40 → 40\n"
     ]
    }
   ],
   "source": [
    "# Install: pip install transformers torch nltk textstat\n",
    "\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "import nltk\n",
    "import textstat\n",
    "\n",
    "# Download the punkt tokenizer for sentence splitting (only once)\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Define your input text\n",
    "text = \"\"\"Pursuant to the overarching objectives articulated within the Strategic Urban Development Framework, municipal authorities have commenced implementation of multifaceted infrastructural initiatives intended to augment both intermodal connectivity and socioeconomic inclusivity. The preliminary phase encompasses the recalibration of extant transportation modalities, the optimization of resource allocation protocols, and the systemic integration of public-private partnerships to facilitate sustainable urban growth trajectories.\"\"\"\n",
    "\n",
    "print(\"🤖 Loading AI simplification model...\")\n",
    "\n",
    "try:\n",
    "    # Choose a model that fits your disk space and needs (\"sshleifer/distilbart-cnn-12-6\" is small and good for summarization)\n",
    "    simplifier = pipeline(\n",
    "        \"text2text-generation\",\n",
    "        model=\"sshleifer/distilbart-cnn-12-6\",\n",
    "        device=0 if torch.cuda.is_available() else -1\n",
    "    )\n",
    "    print(\"✅ Model loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading model: {e}\")\n",
    "    simplifier = None  # Don't use fallback\n",
    "\n",
    "def huggingface_simplify(text):\n",
    "    \"\"\"Simplify text using Hugging Face AI model ONLY\"\"\"\n",
    "    if simplifier is None:\n",
    "        print(\"❌ No model available for simplification.\")\n",
    "        return [text]  # Return original if model fails\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    simplified_sentences = []\n",
    "    for sentence in sentences:\n",
    "        try:\n",
    "            if len(sentence.split()) > 10:\n",
    "                result = simplifier(\n",
    "                    sentence,\n",
    "                    max_length=min(len(sentence.split()) + 10, 100),\n",
    "                    min_length=max(len(sentence.split()) - 5, 10),\n",
    "                    do_sample=True,\n",
    "                    temperature=0.3,\n",
    "                    top_p=0.9\n",
    "                )\n",
    "                simplified = result[0].get('summary_text', sentence)\n",
    "            else:\n",
    "                simplified = sentence\n",
    "            simplified_sentences.append(simplified)\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error processing sentence, using original: {e}\")\n",
    "            simplified_sentences.append(sentence)\n",
    "    return simplified_sentences\n",
    "\n",
    "print(\"\\n=== AI-POWERED DYSLEXIALENS SIMPLIFICATION ===\")\n",
    "print(f\"Original text grade level: {textstat.flesch_kincaid_grade(text):.1f}\")\n",
    "print(f\"Difficult words: {textstat.difficult_words(text)}\")\n",
    "\n",
    "print(\"\\n🤖 Processing with AI model...\")\n",
    "simplified_sentences = huggingface_simplify(text)\n",
    "\n",
    "print(\"\\n=== SIMPLIFIED TEXT ===\")\n",
    "for i, sentence in enumerate(simplified_sentences, 1):\n",
    "    print(f\"{i}. {sentence}\")\n",
    "\n",
    "if simplified_sentences:\n",
    "    final_text = ' '.join(simplified_sentences)\n",
    "    final_grade = textstat.flesch_kincaid_grade(final_text)\n",
    "    improvement = textstat.flesch_kincaid_grade(text) - final_grade\n",
    "    print(f\"\\n=== RESULTS ===\")\n",
    "    print(f\"Original grade level: {textstat.flesch_kincaid_grade(text):.1f}\")\n",
    "    print(f\"Simplified grade level: {final_grade:.1f}\")\n",
    "    print(f\"Improvement: {improvement:.1f} grades easier\")\n",
    "    print(f\"Difficult words reduced: {textstat.difficult_words(text)} → {textstat.difficult_words(final_text)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
